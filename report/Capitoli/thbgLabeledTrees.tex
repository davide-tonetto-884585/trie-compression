\section{Labeled Trees and Tries} \label{chp:thbg_labeled_tree}
Before delving into specific compression techniques, it is essential to establish a solid theoretical foundation regarding labeled trees and, in particular, tries.
These structures are fundamental for representing hierarchical data across diverse fields, from bioinformatics to document processing.
This chapter provides the necessary background, defining these two data structures, exploring their common applications, and introducing the core concepts behind their compression and indexing. 

\subsection{Definition}
\begin{definition}[Labeled tree] \label{def:labeled_tree}
    A \textbf{labeled tree} is a 5-tuple $T = (V, E, \Sigma, \lambda, r)$, where:
    \begin{itemize}
        \item $V$, $E$, and $r$ follow the same definition of the rooted tree (\cref{def:rooted_tree}),
        \item $\Sigma$ is a finite set of labels, called the alphabet.
        %\item $E \subseteq V \times V$ is the set of directed edges such that $(V,E)$ forms a rooted tree,
        \item $\lambda : E \to \Sigma$ is an edge-labeling function.
        %\item $r \in V$ is the root vertex.
    \end{itemize}
\end{definition}
In the case of \emph{ordered} labeled trees, the children of each node are totally ordered, while the degree and shape of the tree, as well as the size of the alphabet $\Sigma$, are unconstrained.


While labeled trees encompass a broad class of hierarchical structures, this thesis focuses specifically on tries.
Tries are a special case of DFAs, restricted to a tree-shaped structure, and are therefore widely used in Computer Science for representing finite sets of strings.
% \alessio{What do you think about making the definition more similar to the DFA? So transition function, etc...}
\begin{definition}[Trie] \label{def:trie}
    A \textbf{trie} is a 6-tuple $T = (V, E, \Sigma, \lambda, r, F)$ where:
    \begin{itemize}
        \item $V$ is a finite set of vertices,
        \item $E \subseteq V \times V$ is a set of edges such that $(V,E)$ forms a rooted tree,
        \item $\Sigma$ is a finite set of labels, called the alphabet.
        \item $\lambda: E \to \Sigma$ is an edge-labeling function,
        \item $r \in V$ is the root vertex,
        \item $F \subseteq V$ is the set of final (accepting) vertices.
    \end{itemize}
\end{definition}
Notice that, when expressing a trie as a DFA, each node should have a child for each character of the alphabet $\Sigma$.
However, for ease of notation and representation, we will only consider nodes and edges that will eventually lead to a final state.
For this reason, our definition of trie is closer to the definition of labeled trees rather than DFAs.

Key properties of a trie include:
\begin{enumerate}
    \item \textbf{Determinism}: For every vertex $v \in V$ and every symbol $a \in \Sigma$, there is at most one edge $(v, u) \in E$ such that $\lambda((v, u)) = a$.
    \item \textbf{String Representation}: For any vertex $v \in V$, let $str(v)$ be the string obtained by concatenating the labels on the unique path from the root $r$ to $v$. We set $str(r) = \varepsilon$.
    \item \textbf{Language Correspondence}: The language represented by the trie is exactly $L$, i.e., $L = \{ str(v) \mid v \in F \}$.
    \item \textbf{Prefix Property}: The set of all prefixes of words in $L$ coincides with $\{ str(v) \mid v \in V \}$.
\end{enumerate}
These properties collectively make tries a powerful data structure. The \textbf{determinism} ensures that for any given string, there is only one path through the trie, making search operations efficient and unambiguous. The \textbf{string representation} and \textbf{language correspondence} properties formally establish the connection between the trie's structure and the set of strings it represents. Finally, the \textbf{prefix property} is fundamental to the trie's utility: it guarantees that every node corresponds to a prefix of at least one word in the language, which is the basis for autocomplete systems and is crucial for compression techniques that exploit shared prefixes.

This thesis addresses the problem of compressing repetitive collections of strings. Tries provide a natural data structure for this task, as compression can be achieved by merging their identical subtrees. Our approach, however, aims to produce not just a minimal automaton, but a $p$--sortable one (\cref{def:p-sorable-automaton}) that also supports efficient indexing. The formal basis for this compression still relies on identifying equivalent states by computing the Myhill--Nerode equivalence classes (see \cref{def:myhill-nerode}), a process related to DFA minimization.

Given that a trie is a special case of DFA, we can apply automaton minimization techniques to compress its structure. 
This process, known as \textbf{DAG compression} (see \cref{sec:hopcroft}) consists of transforming a tree into a directed acyclic graph (DAG) by identifying and merging its isomorphic subtrees. The result is the smallest possible automaton that recognizes the same language.

\subsection{Indexing} \label{compandindexinglabtree}
The goal of compressing and indexing labeled trees is to design a compressed storage scheme for a labeled tree $T$ with $t$ nodes that allows for efficient navigation operations in $T$, as well as fast search and retrieval of subtrees or paths within $T$. To be effective, the compressed representation should minimize the space required to store the tree while supporting a wide range of operations in optimal (i.e., $O(1)$) or (near-)optimal time.

We focus on the following operations on labeled trees, as they will be useful in future sections:
\begin{definition}[Tree Operations] \label{def:tree_operations}
Given a labeled tree $T = (V, E, \lambda, r)$, a node $u \in V$, and a symbol $s \in \Sigma$, we define the following fundamental operations:

\begin{itemize}
    \item \textbf{Navigational queries:} ask for the parent of $u$, the $i$-th child of $u$, or the label of $u$. The last two operations might be restricted to the children of $u$ with a specific label $s$.
    \item \textbf{Visualization queries:} retrieve the nodes in the subtree rooted at $u$ for a given order (such as depth-first).
    \item \textbf{Subpath queries:} given a word $\alpha \in \Sigma^*$, answer:
    %\nicola{Nota: le subpath queries erano tutte definite in modo sbagliato perché partivi sempre da $q_0$. Devi invece partire da un $q\in Q$ generico per catturare cammini arbitrari (che non iniziano necessariamente in $q_0$). Ricontrollare inconsistenze in tutta la tesi. }
    \begin{itemize}
        \item Existence: determine whether $\hat\delta(q,\alpha) \neq \emptyset$ for some $q\in Q$, i.e., check if there exists a path in $T$
        labeled with string $\alpha$.
        \item Counting: compute the size of $\bigcup_{q\in Q} \hat\delta(q,\alpha)$.
        \item Locating: return $\bigcup_{q\in Q} \hat\delta(q,\alpha)$.
    \end{itemize}
\end{itemize}
\end{definition}

A naive solution to index labeled trees is to store the tree as a list of nodes with their labels and parent-child relationships using pointers in $O(t \log t)$ bits. However, this representation is not space-efficient and does not support fast subpath query operations.
In order to implement fast, compressed tree indexes, we first have to understand which are the information-theoretic lower bounds for their lossless representation.
\begin{comment}
    \section{Succinct Data Structures for Trees}
    \alessio{Questa sezione non è ben collegata al resto. Serve questa parentesi sulle SDS? Dato che anche prima parli di minimizzare lo spazio e time optimality, potresti parlare direttamente dei lower bound. Il filo logico sarebbe: "vogliamo fare le cose il meglio possibile, ma quanto vale il meglio?"}
    In order to compress the index of labeled trees, we need to avoid the use of pointers and store the tree in a space-efficient manner. Succinct data structures are a class of compressed data structures that support efficient navigation and query operations on the compressed data. These structures are designed to use close to the information-theoretic lower bound on space while providing fast access to the original data. They were first introduced by Jacobson \cite{jacobson1989space} and have been applied to various problems in string processing, graph theory, and data compression.
\end{comment}
%\subsection{Information-Theoretic Lower Bound }
To do so, it is essential to understand the concept of \emph{worst-case entropy}, which is a formal measure of the minimum number of bits required to represent any object in a given set.
As detailed by Navarro\cite{navarro2016compact}, this is a fundamental concept in data structure design.

\begin{definition}[Worst-case entropy]
Let $U$ be a universe of combinatorial objects. The worst-case entropy of $U$ is 
$$ H_{wc}(|U|) = \lceil\log_2(|U|)\rceil $$
\end{definition}

This definition establishes that the theoretical minimum number of bits needed to uniquely identify any object in a set $U$ is the logarithm of the size of $U$, rounded up to the nearest integer. We now apply this principle to determine the lower bound for labeled trees.

\begin{lemma} \label{lem:info_theoretic_lower_bound}
The information-theoretic lower bound for storing an edge-labeled tree $T$ with $t$ nodes and $m$ edges over an alphabet $\Sigma$ is $2t + m \log_2 |\Sigma| - \Theta(\log t)$ bits.
\end{lemma}

\begin{proof}
The total information required to store a labeled tree can be decomposed into two components: the space needed to encode the tree's structure and the space needed to encode the labels on its nodes.

\textbf{1. Structural Information (Unlabeled Tree):}
The number of distinct unlabeled binary trees with $t$ nodes is given by the $t$-th Catalan number $C_t = \frac{1}{t+1} \binom{2t}{t}$. Using Stirling's approximation for factorials, the Catalan number can be approximated as:
$$C_t \leq \frac{4^t}{t^{3/2}\sqrt{\pi}} ~.$$
Then, the worst-case entropy to encode the structure of the tree is:
$$\log_2 C_t \leq \log_2\left(\frac{4^t}{t^{3/2}\sqrt{\pi}}\right) = \log_2(4^t) - \log_2(t^{3/2}\sqrt{\pi}) = 2t - \frac{3}{2}\log_2 t +\frac{1}{2}\log_2 \pi ~.$$
The lower-order terms can be expressed using big Theta notation as $\Theta(\log t)$. Therefore, the space required for the structure is $2t - \Theta(\log t)$ bits.

\textbf{2. Labeling Information:}
For a tree with $m$ edges and an alphabet $\Sigma$, each edge must be assigned a label. To distinguish between $|\Sigma|$ possible labels, a minimum of $\log_2 |\Sigma|$ bits is required for each edge. Consequently, the total space required to store the labels for all $m$ edges is:
$$m \log_2 |\Sigma| \text{ bits.}$$

Finally, by adding the space required for the structure and the labels, the total information-theoretic lower bound for storing a labeled tree is the sum of the two components is
$$ (2t - \Theta(\log t)) + (m \log_2 |\Sigma|) = 2t + m \log_2 |\Sigma| - \Theta(\log t) \text{ bits.} $$
\end{proof}


%\nicola{va anche detto che nella tua tesi ti focalizzi sui tries, mentre questo lower bound è per i trees. Dai un'occhiata a \url{https://arxiv.org/pdf/2507.02728}, menzionano questo bound (o chiedi direttamente a Carlo) }

\subsection{Indexing and Compressing Labeled Trees: State of The Art} \label{sec:state-of-the-art}

The field of tree indexing and compression has evolved through two main paradigms: succinct data structures that achieve space-optimal representations, and compression techniques that exploit structural repetitions. Neither paradigm has surpassed the other because they excel in different scenarios. 

Succinct data structures provide worst-case space guarantees, aiming to store the tree in a number of bits close to the information-theoretic lower bound. This makes them highly effective for arbitrary trees with little to no repetition. On the other hand, compression techniques that exploit structural repetitions—such as representing a trie as a Directed Acyclic Graph (DAG) by merging identical subtrees—can achieve significantly better compression on datasets with high redundancy. Their effectiveness, however, is entirely dependent on the repetitiveness of the input data, and they may provide little benefit for non-repetitive structures. The choice between them depends on the expected characteristics of the data.
As a consequence, many data structures have been proposed to compress and index labeled trees, each with its trade-offs in terms of space usage, query performance, and supported operations. 

In the realm of succinct tree structures, early work by Kosaraju~\cite{kosaraju1989efficient} proposed a method to index labeled trees by extending the concept of prefix sorting from strings to labeled trees using trie structures. He introduced the idea of constructing a suffix tree for a reversed trie, enabling subpath queries in $O(|P|\log|\Sigma|+ occ)$ time, where $occ$ is the number of occurrences of $P$ in $T$. However, this approach still required $O(t \log t)$ bits of space
% (where $t$ is the number of nodes of the tree)
and thus was not compressed. 

A significant advancement in this direction came with the Extended Burrows-Wheeler Transform (XBWT)~\cite{ferragina2009compressing}, a data structure designed for efficient compression and indexing of ordered node-labeled trees. 
The XBWT works by linearizing a labeled tree into two arrays, one capturing the structural properties of the tree and the other its labels. 
This transformation allows for a space-efficient representation, while being able to perform queries within (near-)optimal time bounds. 
%The key advantage of the XBWT lies in its ability to compress labeled trees while supporting a wide range of operations, such as navigation, visualization and subpath queries , within (near-)optimal time bounds and entropy-bounded space.
The XBWT provides significant improvements in both compression ratio and query performance compared to traditional compression schemes, making it a valuable resource for intensive applications.
We will study in detail this data structure in \cref{sec:XBWT}.

Complementing succinct approaches, tree compression has been extensively studied through different techniques that exploit structural repetitions in distinct ways. One of the classical approaches is \emph{DAG compression} (equivalent to DFA minimization by interpreting the tree as a Finite State Automaton), which represents a tree as a minimal DAG by identifying and merging identical rooted subtrees. 
Concretely, whenever two identical subtrees occur, only one copy is stored and all other occurrences are represented as pointers to it. 
The resulting structure can be exponentially smaller than the original tree and can be computed in linear time. DAG compression is widely used in programming languages, binary decision diagrams, and XML representations~\cite{billeTreeCompressionTop2015}.

Another line of research extends the well-known LZ77 factorization from strings to trees. An LZ77 factorisation of a word $w$ is a representation $w = f_1f_2 \cdots f_\ell$, where each phrase $f_i$ is either a single letter or it has already occurred in the text. Formally, $f_i = w[j \dots j + |f_i| - 1]$ for some $j \le |f_1 \cdots f_{i-1}|$. If $j + |f_i| > |f_1 \cdots f_{i-1}|$, i.e.\ the whole phrase occurred earlier, the factorisation is called self-referencing; otherwise, it is non-self-referencing. Self-referencing factorisations can be more succinct, as with the string $a^k$. Each phrase $f_i$ can be represented as either a single letter or a pair $(j, |f_i|)$, yielding a compressed representation whose size is $\ell$. The smallest LZ77 factorisation of a text can be computed in linear time. This concept can be extended to trees. Here, the tree is decomposed into edge-disjoint fragments, where each fragment is either a single node or a copy of a fragment that appeared earlier in a breadth-first traversal. Each fragment is thus defined by a pointer to an earlier occurrence, much like in the string version of LZ77. This factorization uniquely determines the tree, and by minimizing the number of fragments one obtains a compressed representation. Importantly, such factorizations can be computed in polynomial time (and in linear time for some restricted variants), yielding representations no larger than the smallest tree grammar, thus bridging block compression and grammar-based compression~\cite{gawrychowskiLZ77FactorisationTrees2016}.

More recently, top tree compression has been proposed as a method that combines the advantages of subtree sharing and grammar-like approaches. The key idea is to build a hierarchical top tree decomposition, where the input tree is recursively partitioned into clusters that capture connected patterns. These clusters are then merged following a restricted set of operations, producing a binary decomposition tree whose internal repetitions are turned into subtree repeats. Finally, this decomposition is compressed using standard DAG compression, resulting in a so-called top DAG. This approach achieves close-to-optimal worst-case bounds, can be exponentially more succinct than DAG compression, and crucially, supports a wide range of navigational queries (e.g., parent, child, depth, nearest common ancestor) in logarithmic time directly on the compressed representation~\cite{billeTreeCompressionTop2015}.

Another notable approach is Tree Re-Pair~\cite{lohrey2011tree}, a grammar-based compression technique adapted for tree structures. It extends the principles of the original Re-Pair algorithm~\cite{larsson2000off} to handle the hierarchical nature of trees by identifying and compactly representing frequently occurring patterns as non-terminal characters in the grammar decomposition.
The process involves the linearization of the tree and then the application of the Re-Pair logic, which finds the most frequent pair of adjacent elements (which could represent nodes, labels, or structural components, depending on the linearization) in the sequence, and replaces it with a new non-terminal symbol, and the corresponding production rule is added to the resulting grammar. 
All this process is then repeated until no more pairs occur frequently enough or some other stopping criterion is met. The final output is a relatively small grammar and a sequence of symbols (including the newly introduced non-terminals) that can be used to reconstruct the original tree. An application of Tree Re-Pair to XML documents can be found in~\cite{lohrey2013xml}.

In summary, DAG compression is efficient but limited to subtree repeats, LZ77 factorisation captures more general repetitions while relating closely to grammar-based methods such as Tree Re-Pair, and top tree compression strikes a balance by exploiting both subtree and pattern repeats while still enabling efficient query support.

This thesis focuses on developing a novel technique specifically tailored for highly repetitive tries. Our approach leverages the structural properties unique to tries and their repetitive patterns. Therefore, we use the XBWT as our primary benchmark for comparison, as it represents a well-established and high-performance baseline specifically designed for trie compression in the field.
