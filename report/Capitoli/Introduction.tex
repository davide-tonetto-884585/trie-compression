\chapter{Introduction} \label{chp:introduction}
\section{Project Overview}
The increasing availability of large, structured datasets, such as those found in XML documents, biological data, and hierarchical knowledge bases, has led to the need for efficient compression techniques for trees.
Trees are a natural choice for representing hierarchical data due to their ability to model parent-child relationships and nested structures. For instance, an XML document is inherently a tree, where tags are nested to create a structured hierarchy. Similarly, file systems are organized as trees of directories and files, and biological data, such as phylogenetic trees, use this structure to represent evolutionary relationships. Given their ubiquity in representing complex data, developing effective compression methods for trees is of paramount importance.
Traditional compression methods, such as general-purpose text compression algorithms, often fail to effectively exploit the hierarchical structure of trees. Consequently, specialized tree compression techniques have been developed to address this issue.

Among the most prominent techniques for tree compression, the \textit{Extended Burrows-Wheeler Transform (XBWT)} \cite{ferragina2009compressing} extends the classical Burrows-Wheeler Transform to labeled trees, leveraging their structural properties to achieve significant compression. Another notable approach includes \textit{Re-Pair-based compression} \cite{lohrey2011tree}, which applies grammar-based compression to the tree structure. 

Despite these advancements, existing techniques may not be optimal when dealing with trees characterized by a high degree of repetitiveness. Many real-world datasets, such as versioned documents or biological phylogenies, contain repeated substructures that can be exploited to achieve better compression. This thesis aims to study a novel compression technique designed to efficiently handle such highly repetitive tries. We implement and evaluate this method, comparing it with existing state of the art approaches to determine its effectiveness in different scenarios.

\section{Challenges and Contributions}
In order to develop an effective trie compression scheme that can exploit repetitive structures, we need to address several key challenges:
\begin{itemize}
    \item \textbf{Identification of repetitive structures:} The first step in compressing repetitive tries is to identify the repeated substructures efficiently. This requires the development of algorithms capable of detecting and representing these structures compactly.
    \item \textbf{Optimization of representation:} Once the repetitive structures have been identified, the challenge is to represent them in an optimized way that minimizes the overall size of the compressed trie. This involves finding the most efficient encoding for the repeated substructures.
\end{itemize}

We address these challenges by developing a novel trie compression scheme that first leverages the well-known deterministic finite automaton (DFA, \ref{def:dfa}) minimization algorithm \cite{revuz1992minimisation} to identify repetitive structures. This algorithm efficiently groups together similar subtries, enabling us to identify and compress them with high efficiency. The trie is treated as a DFA where the root is the initial state and the leaves are the final states. Since tries are acyclic graphs, this structure is a specific type of DFA known as a Directed Acyclic Word Graph. For this reason, we focus on an adaptation of Revuz's algorithm \cite{revuz1992minimisation}, which is specifically designed for minimizing acyclic DFAs, to make it more efficient for our purposes.

Then, we optimize the representation of these structures. Our approach is to partition the trie nodes into chains and minimize the number of consecutive nodes with the same equivalence class in each chain. The key challenge is to create partitions that maximize the effectiveness of this optimization. We prove that this optimization problem can be reduced to the Minimum Weight Perfect Bipartite Matching (MWPBM) problem. MWPBM is a classic graph theory problem focused on finding a pairing of all nodes in a bipartite graph such that the sum of the weights of the connecting edges is minimized. By modeling our partitioning problem as a bipartite graph, we can use efficient algorithms for MWPBM to find the optimal representation and achieve a higher compression ratio.

Once the optimal chains are determined, we further compress each chain by collapsing consecutive nodes that belong to the same equivalence class. Specifically, any sequence of consecutive nodes within the same chain that share the same equivalence class is merged into a single representative node. This collapsed node preserves the connectivity of the original structure by inheriting all distinct outgoing and incoming edges from the nodes it replaces. This collapsing operation transforms the original deterministic structure into a DFA or a non-deterministic finite automaton (NFA, \cref{def:nfa}) while preserving the language recognized by the original trie. The combination of optimal chain partitioning and node collapsing significantly reduces the space complexity of the data structure.

\section{Structure of The Thesis}
% TODO: rifare una volta ristrutturati i capitoli
This thesis is structured to guide the reader from the foundational concepts of trie compression to the development and evaluation of our novel approach. The goal is to build a clear understanding of why each component of our proposed pipeline is necessary and how they fit together.

The logical flow is as follows:
\begin{itemize}
    \item We begin in \textbf{\cref{chp:thbg_labeled_tree}} by establishing the necessary theoretical background on labeled trees.
    \item In \textbf{\cref{chp:tree_compression}}, we examine the Extended Burrows-Wheeler Transform (XBWT), a state-of-the-art tree compression technique. This serves as a benchmark and highlights the opportunity for improvement, particularly in handling highly repetitive structures.
    \item To address this, we introduce a new approach based on automata. \textbf{\cref{chp:hopcroft}} describes how we use Deterministic Finite Automata (DFA) to model the trie's structure and apply Hopcroft's algorithm to minimize this automaton, effectively identifying all unique subtries (i.e., the repetitions).
    \item Once repetitions are identified, we need an efficient way to store them. \textbf{\cref{chp:min_weight_perfect_bipartite_matching}} introduces the Minimum Weight Perfect Bipartite Matching problem, which we use to find an optimal way to chain the identified repetitive structures, minimizing the overall compressed size.
    \item \textbf{\cref{chp:project_overview}} unites these concepts, presenting the complete pipeline of our proposed compression scheme.
    \item Finally, \textbf{\cref{chp:implementation,chp:experimental_results,chp:conclusions}} describe the implementation, present the experimental results of our method against the benchmark, and discuss our conclusions and future work.
\end{itemize}
