\chapter{Introduction} \label{chp:introduction}
The problem of compressing large sets of strings, or finite languages, is a fundamental challenge in computer science with applications in areas like bioinformatics, natural language processing, and data indexing. A finite language can be naturally represented by an acyclic deterministic finite automaton (ADFA), commonly known as a trie. In this representation, each string in the language corresponds to a unique path from the root to a final state. Compressing the language is therefore equivalent to compressing its corresponding trie structure.

Traditional compression algorithms often fail to exploit the inherent structural properties of tries. To address this, specialized techniques have been developed. Among the most prominent is the \textit{Extended Burrows-Wheeler Transform (XBWT)} \cite{ferragina2009compressing}, which extends the classical Burrows-Wheeler Transform \cite{burrows1994block} to labeled trees and can be applied to tries to achieve significant compression by capturing their structural regularities.

However, existing techniques may not be optimal when dealing with tries that exhibit a high degree of repetitiveness. Such is the case for languages containing many strings with shared substrings, leading to tries with large, identical subtrees. Many real-world datasets, such as genomic databases or dictionaries of related terms, generate such highly repetitive structures. This thesis introduces and analyzes a novel compression technique specifically designed to exploit these repetitions. The core idea is to identify and merge identical subtrees by reducing the trie to its minimal deterministic finite automaton (DFA) representation. We implement this method and evaluate its performance against state-of-the-art approaches like XBWT, assessing its effectiveness on various datasets.

\section{Challenges and Contributions}
The primary goal of this thesis is to develop a data structure that both compresses a given finite language and efficiently supports indexing queries, such as navigational and subpath queries (see \cref{def:tree_operations}). This challenge involves navigating a fundamental trade-off between compression and indexability, which we explore in detail in \cref{sec:wheeler_and_psortable_graphs}. Two straightforward approaches highlight the extremes of this spectrum:

\begin{itemize}
    \item \textbf{Full Compression, Difficult Indexing:} One could minimize the input trie into the smallest possible equivalent DFA using an algorithm like Revuz's \cite{revuz1992minimisation}. While this yields optimal compression through DAG compression (see \cref{sec:notation}), indexing the resulting general DFA is a notoriously difficult problem. As shown by Equi et al. \cite{equiGraphsCannotBe2023}, pattern matching on general DFAs requires super-polynomial time unless the Strong Exponential Time Hypothesis (SETH) fails, making this approach unsuitable for most indexing purposes.
    
    \item \textbf{Full Indexability, No Compression:} At the other extreme, the input trie itself can be used as an index. Tries are Wheeler graphs \cite{gagie2017wheeler}, specifically 1-sortable automata (\cref{def:wheeler_automaton}), a property that makes them highly amenable to efficient indexing \cite{cotumaccio2023co}. While this provides excellent query performance through the co-lexicographic ordering of states (see \cref{def:colex_order_on_automaton}), it offers no compression, as even highly repetitive subtrees are stored explicitly.
\end{itemize}

This thesis proposes a novel algorithm that finds a sweet spot in this trade-off, which we develop throughout \cref{chp:tree_compression}. The central idea is to partially minimize the input trie while ensuring the resulting automaton remains efficiently indexable. We achieve this by leveraging the theory of $p$-sortable graphs (see \cref{sec:wheeler_and_psortable_graphs}), developing a method that strategically increases the sortability parameter $p$ just enough to enable significant compression. The motivation for this approach is rooted in the observation that a small increase in $p$ can lead to substantial compression. As noted by Policriti et al. \cite{manziniRationalConstructionWheeler2024}, there are cases where increasing $p$ from 1 to 2 allows for an exponential reduction in the automaton's size, a phenomenon we explore in detail in \cref{sec:wheeler_and_psortable_graphs}.

Our compression scheme, presented in \cref{chp:tree_compression}, works by partitioning the trie's nodes into a predefined number $p$ of chains and then optimizing this partition to merge the maximum number of equivalent states while ensuring $p$-sortability. To make this optimization problem more concrete, we can frame it as a string partitioning problem. Consider the sequence of nodes in the trie, when read in co-lexicographic order (see \cref{def:colex_order_on_automaton}), as a single long string. The "character" corresponding to each node is its Myhill-Nerode equivalence class (see \cref{def:myhill-nerode}), which determines if it can be merged with other nodes. The task is to partition this string of nodes into $p$ subsequences such that the number of runs is minimized, where a run is a maximal sequence of consecutive nodes of the same equivalence class. For instance, a subsequence $AAABBA$ contains three runs ($AAA$, $BB$, $A$). Minimizing the number of runs directly corresponds to maximizing the number of merged states, yielding a compact $p$-sortable automaton.

In \cref{chp:tree_compression}, we prove that this optimization is equivalent to the Minimum Weight Perfect Bipartite Matching (MWPBM) problem, allowing us to use efficient, well-studied algorithms to find the optimal solution. The result is a compressed automaton that is $p$-sortable by construction and thus supports efficient queries using the data structure developed by Cotumaccio et al. \cite{cotumaccio2023co}. As our experimental results in \cref{chp:experiments} will show, this method is particularly effective for the highly repetitive datasets common in real-world applications, achieving a balance of compression and indexability that prior methods could not attain.

\section{Structure of The Thesis}
