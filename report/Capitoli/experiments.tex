\chapter{Implementation and Experiments} \label{chp:experiments}

\section{Experimental Setup}
All experiments were conducted on a consistent hardware and software platform to ensure fair and reproducible comparisons. This section details the environment and datasets used in our evaluation.

\subsection{Hardware}
The experimental platform is a MacBook Pro equipped with an Apple M4 Pro processor and 24 GB of RAM, running macOS. 

\begin{comment}
In brief, paper ~\cite{kolmogorovBlossomNewImplementation2009} presents Blossom~V, a practical implementation of Edmonds’ blossom algorithm for computing minimum-cost perfect matchings in undirected weighted graphs. While the theoretical worst-case bounds for the blossom family have steadily improved since Edmonds’ original $O(n^2m)$ algorithm, Blossom~V is designed for strong empirical performance rather than new asymptotic guarantees. It combines two ingredients that had proven effective separately in prior work: the variable $\delta$ (variable dual updates) strategy popularized by Blossom~IV, and systematic use of priority queues to efficiently select minimum-slack edges.

Blossom~V targets general (not necessarily bipartite) graphs, and thus directly applies to our bipartite instances as a special case. In our experiments we use the publicly available Blossom~V implementation as a black-box solver to compute minimum-cost perfect matchings for the graphs generated by our compression pipeline.
\end{comment}

\subsection{Datasets}
To systematically evaluate performance under controlled conditions, particularly with respect to repetitiveness, we generated a suite of synthetic labeled tries. Real-world datasets often lack ground-truth information about their repetitive structures, making it difficult to isolate the effects of this property. Our synthetic generation approach allows us to create trees with tunable characteristics.

The tries were generated using a custom Python script that constructs a tree and introduces repetitiveness by randomly copying subtries to different locations. At each step of the tree's growth, with a certain probability, a random existing subtrie is selected and duplicated as a new branch. This ``copy-paste'' mechanism allows us to create complex, highly repetitive structures that mimic patterns found in real-world data.

The generation process was controlled by the following key parameters:
\begin{description}
    \item[Max Branching Factor] The maximum number of children for any node.
    \item[Repetition Probability] The probability of copying an existing subtrie instead of creating a new random branch.
    \item[Subtrie Depth Range] The minimum and maximum depth of subtries eligible for copying.
    \item[Max Nodes] The target maximum number of nodes in the generated trie.
    \item[Alphabet Size] The number of unique characters in the alphabet.
    \item[Seed] The seed used for random number generation to ensure reproducibility.
\end{description}

By varying these parameters, we can produce a diverse range of tries to thoroughly test the limits and behaviors of each compression algorithm.

\section{XBWT Implementation and Experiments}
Before presenting the implementation and the experiments related to our proposed trie compression scheme, it is necessary to detail our implementation of the XBWT introduced in \cref{sec:XBWT}. This implementation serves a dual purpose. Firstly, it provides the \textsc{PathSort} algorithm, a fundamental component required by our compression method. Secondly, it will serve as a benchmark for comparison against our scheme in future work. For now, our focus is on the experiments conducted to evaluate the performance of the \textsc{PathSort} algorithm, particularly its execution time.

\input{Capitoli/XBWT/Implementation}
\input{Capitoli/XBWT/ExperimentsAndConclusions.tex}

\section{Trie Compression Scheme Implementation and Experiments}

\subsection{Implementation}
Our proposed trie compression scheme was implemented in C++ to run some preliminary experiments and it is available at \url{https://github.com/davide-tonetto-884585/trie-compression}. We compiled it using Apple Clang 17. 

In particular, we utilized the \textsc{PathSort} algorithm implementation of the XBWT introduced in \cref{sec:xbwt_impl}. Moreover, we used a C++ implementation by Vladimir Kolmogorov of the minimum cost perfect matching algorithm described in~\cite{kolmogorovBlossomNewImplementation2009}. The implementation is available at \url{https://pub.ista.ac.at/~vnk/software.html}.

\subsection{Compression as a Function of \texorpdfstring{$p$}{p}}
Our first experiment investigates the relationship between the co--lexicographical width, $p$, of the automaton produced by our compression pipeline and the final compression size. Our central hypothesis is that a larger value of $p$ can lead to a better compression ratio. This is motivated by the fact that enforcing the strict total ordering of a Wheeler DFA (which corresponds to $p=1$) can lead to an exponential increase in the number of states compared to a minimal DFA representing the same language~\cite{manziniRationalConstructionWheeler2024} (see \cref{sec:wheeler_and_psortable_graphs}). A $p$--sortable automaton relaxes this strict requirement by partitioning the states into $p$ totally ordered chains. By increasing $p$, we provide more flexibility, allowing the automaton to be represented with fewer states—potentially moving closer to the size of a minimal DFA. We therefore expect that a larger $p$ will result in a smaller automaton, leading to better compression.

To generate automata with varying co--lex widths, we controlled the structural repetitiveness of the input tries using the \texttt{repetition\_probability} parameter in our data generator. We conducted two main experiments:

\begin{enumerate}
    \item \textbf{Low-Repetitiveness Scenario:} We generated a set of $100$ tries with a target size of 100,000 nodes, an alphabet size of $26$ characters and a low repetition probability of $0.2$.
    \item \textbf{High-Repetitiveness Scenario:} We generated a second set of $100$ tries with a target size of 100,000 nodes, an alphabet size of $26$ characters, but with a high repetition probability of $0.8$.
\end{enumerate}

We run our full compression pipeline on each trie with different values of $p$ in range $[1, 15]$. We then report the maximum, mean, and minimum number of nodes and edges obtained after compression. By correlating the measured $p$ with the final compression ratio in both scenarios, we aim to empirically demonstrate that the compression of our method is fundamentally governed by the co--lex width of the resulting automaton.

For each value of $p$, the experimental results are presented as boxplots, which provide a comprehensive statistical summary of the compression performance across all $100$ trials. Each boxplot displays:
\begin{itemize}
    \item The \textbf{median} (middle line): the central value that separates the upper and lower halves of the results.
    \item The \textbf{first quartile (Q1)} and \textbf{third quartile (Q3)} (box boundaries): representing the 25th and 75th percentiles, respectively, with the box containing the middle 50\% of the data.
    \item The \textbf{interquartile range (IQR)}: the spread of the middle 50\% of observations, calculated as Q3 - Q1.
    \item The \textbf{whiskers}: extending to the most extreme data points within 1.5 × IQR from the box boundaries.
    \item \textbf{Outliers}: individual points beyond the whiskers, representing unusually high or low compression ratios.
\end{itemize}
This visualization allows us to assess not only the central tendency of compression performance for each $p$ value, but also the variability and distribution shape of the results across different trie structures. Additionally, the mean compression value for each $p$ is highlighted with a red line plot that is overlaid on the boxplots to show the overall trend. To provide a theoretical baseline, a green horizontal line indicates the mean number of distinct Myhill--Nerode classes across all tries in the experiment. Since the number of states in a minimal DFA for the language is equal to the number of MN--classes, this line represents the minimum possible number of states. Observing the red line trend approaching the green line shows that as $p$ increases, the average number of nodes in the compressed automaton approaches this theoretical minimum.

The results from our experiments, presented in \cref{fig:exp_low_rep} and \cref{fig:exp_high_rep}, strongly confirm our initial hypothesis. The findings are particularly interesting: simply increasing the co--lexicographical width from $p=1$ to $p=2$ is sufficient to halve the number of states in the resulting automaton. Furthermore, the benefits of increasing $p$ are most pronounced for small values. As shown in the plots, the number of states rapidly approaches the theoretical minimum (indicated by the green line), with the compression performance essentially reaching its optimum at around $p=8$ for \cref{fig:exp_low_rep} and around $p=11$ for \cref{fig:exp_high_rep}. This demonstrates that even with a very small co--lexicographical width, our method can achieve near-optimal compression by effectively identifying and merging MN--equivalent nodes.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Immagini/tree_compression_analysis_low.pdf}
    \caption{Experimental results for the \textbf{low-repetition} scenario. The plots show the number of nodes (top) and transitions (bottom) in the compressed automaton as a function of the co--lexicographical width, $p$. Each boxplot illustrates the distribution of results for a given $p$. The red line tracks the mean, showing a rapid decrease that approaches the theoretical minimum number of states (the mean number of MN--classes, shown by the green line).}
    \label{fig:exp_low_rep}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Immagini/tree_compression_analysis_high.pdf}
    \caption{Experimental results for the highly-repetitive scenario. The plots show the number of nodes (top) and transitions (bottom) in the compressed automaton as a function of the co--lexicographical width, $p$. Each boxplot illustrates the distribution of results for a given $p$. The red line tracks the mean, showing a rapid decrease that approaches the theoretical minimum number of states (the mean number of MN--classes, shown by the green line).}
    \label{fig:exp_high_rep}
\end{figure}

To provide a clear comparison between the two scenarios, \cref{fig:exp_comparison} shows the mean and standard deviation for both the low- and high-repetitive datasets. The results confirm our hypothesis: tries with higher trie repetitiveness achieve significantly better compression, as evidenced by the lower number of nodes and transitions for all values of $p$. This is expected, as a more repetitive trie allow for a more compact representation in the compressed automaton.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Immagini/high_low_comparison.pdf}
    \caption{Direct comparison of compression performance for low-repetitive versus high-repetitive datasets. The plots track the mean number of nodes (top) and transitions (bottom) as a function of $p$. The results clearly show that high-repetitive datasets (orange) yield significantly better compression across all values of $p$, resulting in a much smaller final automaton. This highlights the effectiveness of our method in exploiting the redundancy present in highly repetitive data.}
    \label{fig:exp_comparison}
\end{figure}